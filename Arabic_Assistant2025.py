import streamlit as st
import requests
import json
import nltk
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.naive_bayes import MultinomialNB
from langdetect import detect
import time

# Download NLTK punkt tokenizer
try:
    nltk.download('punkt', quiet=True)
except:
    pass

# ---------------------------
# Knowledge Base
# ---------------------------
kb = [
    "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙ‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¢Ù„Ø§Øª Ø°ÙƒÙŠØ© ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªØ¹Ù„Ù… ÙƒØ§Ù„Ø¨Ø´Ø±",
    "ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØªØ­Ø³Ù† Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª",
    "Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ© ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ù„ÙÙ‡Ù…Ù‡Ø§ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨",
    "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© ÙŠØ³ØªØ®Ø¯Ù… Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©",
    "Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ù‡ÙŠ Ø¢Ù„Ø§Øª Ù…Ø¨Ø±Ù…Ø¬Ø© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ù‡Ø§Ù… Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ù„ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª",
    "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© ØªÙˆÙØ± Ø®Ø¯Ù…Ø§Øª Ø­ÙˆØ³Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©",
    "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ ÙŠÙ‡ØªÙ… Ø¨Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ù…Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©",
    "Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø´Ø¨ÙƒØ© Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…ØªØµÙ„Ø© Ø§Ù„ØªÙŠ ØªØ¬Ù…Ø¹ ÙˆØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
    "Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙŠØ®Ù„Ù‚ Ø¨ÙŠØ¦Ø© Ù…Ø­Ø§ÙƒØ§Ø© ÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‡Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ¨Ø¯Ùˆ Ø­Ù‚ÙŠÙ‚ÙŠØ©"
]

# Cache the TF-IDF vectorizer and vectors
@st.cache_resource(show_spinner=False)
def load_vectorizer_and_kb():
    vectorizer = TfidfVectorizer()
    kb_vectors = vectorizer.fit_transform(kb)
    return vectorizer, kb_vectors

vectorizer, kb_vectors = load_vectorizer_and_kb()

# ---------------------------
# Translation API with Auto-Detect
# ---------------------------
def translate_text(text, target_language="en"):
    try:
        source_lang = detect(text)
        if source_lang == target_language:
            return text  # No need to translate
        
        url = "https://libretranslate.de/translate"
        payload = {
            "q": text,
            "source": source_lang,
            "target": target_language,
            "format": "text"
        }
        response = requests.post(url, data=payload)
        if response.status_code == 200:
            return response.json()["translatedText"]
        else:
            return f"Translation failed: {response.status_code}"
    except Exception as e:
        return f"Translation error: {e}"

# ---------------------------
# Summarization (TextRank and fallback)
# ---------------------------
def summarize_text(text, num_sentences=3):
    try:
        from sumy.parsers.plaintext import PlaintextParser
        from sumy.nlp.tokenizers import Tokenizer
        from sumy.summarizers.lex_rank import LexRankSummarizer

        parser = PlaintextParser.from_string(text, Tokenizer("arabic"))
        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document, num_sentences)

        return " ".join([str(sentence) for sentence in summary]) or basic_summarizer(text)

    except Exception:
        return basic_summarizer(text)

def basic_summarizer(text, max_words=30):
    words = text.split()
    return " ".join(words[:max_words]) + "..." if len(words) > max_words else text

# ---------------------------
# Sentiment Analysis
# ---------------------------
positive_words = ["Ø³Ø¹ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "Ø¬Ù…ÙŠÙ„", "Ø±Ø§Ø¦Ø¹", "Ù…Ø¨ØªÙ‡Ø¬", "Ø­Ø¨", "Ù†Ø¬Ø§Ø­"]
negative_words = ["Ø­Ø²ÙŠÙ†", "Ø³ÙŠØ¡", "ØºØ§Ø¶Ø¨", "Ù…Ù…Ù„", "Ù‚Ù„Ù‚", "Ù…Ø­Ø¨Ø·", "Ù…ØªØ¹Ø¨"]

def analyze_sentiment(text):
    text = text.lower()
    pos = sum(word in text for word in positive_words)
    neg = sum(word in text for word in negative_words)

    if pos > neg:
        return f"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ (Ø«Ù‚Ø©: {int((pos / (pos + neg + 0.1)) * 100)}%)"
    elif neg > pos:
        return f"Ø³Ù„Ø¨ÙŠ (Ø«Ù‚Ø©: {int((neg / (pos + neg + 0.1)) * 100)}%)"
    return "Ù…Ø­Ø§ÙŠØ¯"

# ---------------------------
# Dialect Detection
# ---------------------------
dialect_texts = ["Ø¥Ø²ÙŠÙƒ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ", "Ø´Ù„ÙˆÙ†Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ", "ÙƒÙŠÙÙƒ Ø´Ùˆ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŸ"]
dialect_labels = ["Ù…ØµØ±ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©"]

@st.cache_resource(show_spinner=False)
def train_dialect_model():
    vec = TfidfVectorizer(ngram_range=(1, 3))
    X = vec.fit_transform(dialect_texts)
    clf = MultinomialNB()
    clf.fit(X, dialect_labels)
    return vec, clf

dialect_vectorizer, dialect_clf = train_dialect_model()

def detect_dialect(text):
    try:
        text_vec = dialect_vectorizer.transform([text])
        pred = dialect_clf.predict(text_vec)[0]
        confidence = max(dialect_clf.predict_proba(text_vec)[0])
        return f"{pred} (Ø«Ù‚Ø©: {int(confidence * 100)}%)"
    except:
        return "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©"

# ---------------------------
# Knowledge Retrieval (RAG)
# ---------------------------
def knowledge_retrieval(query, top_k=2):
    query_vec = vectorizer.transform([query])
    scores = cosine_similarity(query_vec, kb_vectors)[0]
    top_indices = scores.argsort()[-top_k:][::-1]

    if scores[top_indices[0]] > 0.1:
        return "\n\n".join([kb[i] for i in top_indices])
    return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."

# ---------------------------
# Streamlit App
# ---------------------------
st.set_page_config(page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±...")

        # Handle commands
        response = ""
        if prompt.startswith("ØªØ±Ø¬Ù…"):
            text = prompt.replace("ØªØ±Ø¬Ù…", "").strip()
            response = f"**Ø§Ù„ØªØ±Ø¬Ù…Ø©:**\n\n{translate_text(text)}"
        elif prompt.startswith("Ù„Ø®Øµ") or prompt.startswith("Ù…Ù„Ø®Øµ"):
            text = prompt.replace("Ù„Ø®Øµ", "").replace("Ù…Ù„Ø®Øµ", "").strip()
            response = f"**Ø§Ù„Ù…Ù„Ø®Øµ:**\n\n{summarize_text(text)}"
        elif any(word in prompt for word in ["Ø´Ø¹ÙˆØ±", "Ù…Ø´Ø§Ø¹Ø±", "Ø­Ù„Ù„"]):
            response = f"**ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:**\n\n{analyze_sentiment(prompt)}"
        elif "Ù„Ù‡Ø¬Ø©" in prompt:
            response = f"**Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©:**\n\n{detect_dialect(prompt)}"
        else:
            response = f"**Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:**\n\n{knowledge_retrieval(prompt)}"

        # Simulate typing
        for i in range(0, len(response), 10):
            placeholder.markdown(response[:i+10])
            time.sleep(0.05)

        placeholder.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})

with st.sidebar:
    st.header("ğŸŒŸ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯")
    st.markdown("""
    **ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù€:**
    - ğŸ”¤ Ø§Ù„ØªØ±Ø¬Ù…Ø© (ØªÙ„Ù‚Ø§Ø¦ÙŠ)
    - ğŸ“ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ
    - ğŸ’¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
    - ğŸ—£ï¸ ÙƒØ´Ù Ø§Ù„Ù„Ù‡Ø¬Ø©
    - ğŸ“š Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ©
    """)
    
    st.header("ğŸ§  Ù‡Ù„ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø©ØŸ")
    st.button("ğŸ‘ Ù†Ø¹Ù…")
    st.button("ğŸ‘ Ù„Ø§")
