import streamlit as st
import requests
import json
import nltk
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.naive_bayes import MultinomialNB
import time

# Try to download nltk data, but with a fallback
try:
    nltk.download('punkt', quiet=True)
except:
    pass  # Silently continue if download fails

# ---------------------------
# Knowledge Base (Expand this with more entries for better coverage)
# ---------------------------
kb = [
    "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙ‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¢Ù„Ø§Øª Ø°ÙƒÙŠØ© ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªØ¹Ù„Ù… ÙƒØ§Ù„Ø¨Ø´Ø±",
    "ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØªØ­Ø³Ù† Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª",
    "Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ© ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ù„ÙÙ‡Ù…Ù‡Ø§ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨",
    "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© ÙŠØ³ØªØ®Ø¯Ù… Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©",
    "Ø§Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª Ù‡ÙŠ Ø¢Ù„Ø§Øª Ù…Ø¨Ø±Ù…Ø¬Ø© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ù‡Ø§Ù… Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ù„ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª",
    "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© ØªÙˆÙØ± Ø®Ø¯Ù…Ø§Øª Ø­ÙˆØ³Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©",
    "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ ÙŠÙ‡ØªÙ… Ø¨Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª ÙˆØ§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ù…Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©",
    "Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø´Ø¨ÙƒØ© Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…ØªØµÙ„Ø© Ø§Ù„ØªÙŠ ØªØ¬Ù…Ø¹ ÙˆØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
    "Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙŠØ®Ù„Ù‚ Ø¨ÙŠØ¦Ø© Ù…Ø­Ø§ÙƒØ§Ø© ÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‡Ø§ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ¨Ø¯Ùˆ Ø­Ù‚ÙŠÙ‚ÙŠØ©"
]

# Initialize TF-IDF for RAG
try:
    vectorizer = TfidfVectorizer()
    kb_vectors = vectorizer.fit_transform(kb)
except Exception as e:
    st.error(f"Error initializing TF-IDF: {e}")

# ---------------------------
# External Translation API
# ---------------------------
def translate_text(text, target_language="en"):
    """
    Translate text using LibreTranslate API which is free and open-source
    """
    try:
        # Use LibreTranslate's public API (free but may have limitations)
        url = "https://libretranslate.de/translate"
        
        payload = {
            "q": text,
            "source": "ar",
            "target": target_language,
            "format": "text"
        }
        
        response = requests.post(url, data=payload)
        if response.status_code == 200:
            return response.json()["translatedText"]
        else:
            # Fallback to dictionary for common phrases
            if text in translations:
                return translations[text]
            return f"Translation API error: {response.status_code}"
    except Exception as e:
        # Fallback to dictionary
        if text in translations:
            return translations[text]
        return f"Translation error: {str(e)}"

# Fallback translation dictionary
translations = {
    "Ù…Ø±Ø­Ø¨Ø§": "Hello",
    "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "How are you",
    "Ø´ÙƒØ±Ø§": "Thank you",
    "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ": "Artificial Intelligence",
    "ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©": "Machine Learning",
    "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©": "Natural Language Processing",
    "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©": "Big Data",
    "Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡": "Internet of Things",
    "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©": "Cloud Computing",
    "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ": "Cybersecurity"
}

# ---------------------------
# Advanced Summarization Using TextRank Algorithm
# ---------------------------
def summarize_text(text, num_sentences=3):
    try:
        from sumy.parsers.plaintext import PlaintextParser
        from sumy.nlp.tokenizers import Tokenizer
        from sumy.summarizers.lex_rank import LexRankSummarizer
        
        parser = PlaintextParser.from_string(text, Tokenizer("arabic"))
        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document, num_sentences)
        
        if summary:
            return " ".join([str(sentence) for sentence in summary])
        else:
            return basic_summarizer(text)
    except Exception as e:
        # Fallback to basic summarizer
        return basic_summarizer(text)

# Basic summarizer as fallback
def basic_summarizer(text, max_words=30):
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + "..."

# ---------------------------
# Enhanced Sentiment Analysis
# ---------------------------
# Expanded word lists for better sentiment detection
positive_words = [
    "Ø³Ø¹ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "Ø¬Ù…ÙŠÙ„", "Ø±Ø§Ø¦Ø¹", "Ù…Ø¨Ø³ÙˆØ·", "Ø¬ÙŠØ¯", "Ù…Ø³Ø±ÙˆØ±", "Ø±Ø¶Ø§", "ÙØ±Ø­", "Ø­Ø¨",
    "Ù†Ø¬Ø§Ø­", "Ø¥Ù†Ø¬Ø§Ø²", "Ù…ØªÙØ§Ø¦Ù„", "Ø£Ù…Ù„", "Ù…Ø³ØªÙ…ØªØ¹", "Ù…Ø¨ØªÙ‡Ø¬", "Ù…ØªØ­Ù…Ø³", "Ù†Ø§Ø¬Ø­", "Ù…Ø­Ø¨ÙˆØ¨",
    "Ù…Ù‚Ø¯Ø±", "Ù…ØªØ£Ù„Ù‚", "Ù…Ø«ÙŠØ±", "ÙˆØ¯ÙˆØ¯", "Ù…Ø±ÙŠØ­", "Ù…Ø±Ø¶ÙŠ", "Ø¹Ø¸ÙŠÙ…", "Ù…Ø´Ø¬Ø¹", "Ù…Ø­ÙØ²"
]

negative_words = [
    "Ø­Ø²ÙŠÙ†", "Ø³ÙŠØ¡", "ÙƒØ¦ÙŠØ¨", "Ù…Ù…Ù„", "ØºØ§Ø¶Ø¨", "Ù…Ø¶Ø·Ø±Ø¨", "Ù‚Ù„Ù‚", "Ø®Ø§Ø¦Ù", "Ù…Ø­Ø¨Ø·", "Ù…ØªØ¹Ø¨",
    "ÙØ§Ø´Ù„", "Ù…Ø¤Ù„Ù…", "Ù…Ø®ÙŠÙ", "ØµØ¹Ø¨", "Ù…Ø²Ø¹Ø¬", "Ù…Ø±Ù‡Ù‚", "Ù…Ø¶Ø§ÙŠÙ‚", "Ù…ØªØ¶Ø§ÙŠÙ‚", "Ù…Ø­Ø±Ø¬",
    "Ù…Ù‡Ù…ÙˆÙ…", "Ù…Ø´ÙˆØ´", "Ù…ØªÙˆØªØ±", "ÙŠØ§Ø¦Ø³", "Ù…Ø®ÙŠØ¨", "Ù…Ù‡Ø¯Ø¯", "Ù…Ø³ØªØ§Ø¡", "Ù…ØªØ¶Ø±Ø±", "Ù…Ø­Ø²Ù†"
]

def analyze_sentiment(text):
    """Enhanced sentiment analysis with more terms and better scoring"""
    text = text.lower()
    
    # Count occurrences of positive and negative words
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    # Determine sentiment based on count
    if pos_count > neg_count:
        confidence = min(100, int((pos_count / (pos_count + neg_count + 0.1)) * 100))
        return f"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ (Ø«Ù‚Ø©: {confidence}%)"
    elif neg_count > pos_count:
        confidence = min(100, int((neg_count / (pos_count + neg_count + 0.1)) * 100))
        return f"Ø³Ù„Ø¨ÙŠ (Ø«Ù‚Ø©: {confidence}%)"
    else:
        return "Ù…Ø­Ø§ÙŠØ¯"

# ---------------------------
# Improved Dialect Identification
# ---------------------------
dialect_texts = [
    # Egyptian dialect examples
    "Ø¥Ø²ÙŠÙƒ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ", "Ø£Ù†Ø§ Ù…Ø´ Ø¹Ø§Ø±Ù", "Ø¯Ù„ÙˆÙ‚ØªÙŠ", "Ø¹Ø§ÙŠØ²", "Ø¨Øµ ÙƒØ¯Ø§", "Ù…Ø§Ø´ÙŠ", 
    # Gulf dialect examples
    "Ø´Ù„ÙˆÙ†Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ", "ÙŠØ¨ØºØ§Ù„ÙŠ", "ÙˆØ´ ÙÙŠÙƒ", "ÙˆÙŠÙ†ÙƒØŸ", "Ù…Ù† ÙˆÙŠÙ†ØŸ", "Ø·ÙŠØ¨",
    # Levantine dialect examples
    "ÙƒÙŠÙÙƒ Ø´Ùˆ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŸ", "Ø¨Ø¯ÙŠ Ø±ÙˆØ­", "Ù‡ÙŠÙƒ", "Ù…Ù†ÙŠØ­", "Ø­ÙƒÙŠ", "Ø¨ÙƒØ±Ø§"
]

dialect_labels = [
    "Ù…ØµØ±ÙŠØ©", "Ù…ØµØ±ÙŠØ©", "Ù…ØµØ±ÙŠØ©", "Ù…ØµØ±ÙŠØ©", "Ù…ØµØ±ÙŠØ©", "Ù…ØµØ±ÙŠØ©",
    "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©", "Ø®Ù„ÙŠØ¬ÙŠØ©",
    "Ø´Ø§Ù…ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©", "Ø´Ø§Ù…ÙŠØ©"
]

try:
    dialect_vectorizer = TfidfVectorizer(ngram_range=(1, 3))
    X = dialect_vectorizer.fit_transform(dialect_texts)
    dialect_clf = MultinomialNB()
    dialect_clf.fit(X, dialect_labels)
except Exception as e:
    st.error(f"Error initializing dialect classifier: {e}")

def detect_dialect(text):
    """Detect Arabic dialect with confidence score"""
    try:
        text_vec = dialect_vectorizer.transform([text])
        predicted_dialect = dialect_clf.predict(text_vec)[0]
        proba = max(dialect_clf.predict_proba(text_vec)[0])
        confidence = int(proba * 100)
        return f"{predicted_dialect} (Ø«Ù‚Ø©: {confidence}%)"
    except:
        # If classification fails, do basic keyword matching
        text = text.lower()
        if any(word in text for word in ["Ø¥Ø²Ø§ÙŠ", "Ø¥Ø²ÙŠÙƒ", "ÙƒØ¯Ù‡", "Ø¯Ù„ÙˆÙ‚ØªÙŠ", "Ø¹Ø§ÙŠØ²"]):
            return "Ù…ØµØ±ÙŠØ© (ØªØ®Ù…ÙŠÙ† Ø¨Ø³ÙŠØ·)"
        elif any(word in text for word in ["Ø´Ù„ÙˆÙ†", "ÙŠØ¨ØºÙ‰", "ÙˆØ´", "ÙˆÙŠÙ†"]):
            return "Ø®Ù„ÙŠØ¬ÙŠØ© (ØªØ®Ù…ÙŠÙ† Ø¨Ø³ÙŠØ·)"
        elif any(word in text for word in ["ÙƒÙŠÙ", "Ù‡ÙŠÙƒ", "Ø¨Ø¯ÙŠ", "Ù…Ù†ÙŠØ­"]):
            return "Ø´Ø§Ù…ÙŠØ© (ØªØ®Ù…ÙŠÙ† Ø¨Ø³ÙŠØ·)"
        return "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©"

# ---------------------------
# Enhanced RAG for Knowledge Retrieval
# ---------------------------
def knowledge_retrieval(query, top_k=2):
    """Retrieve the most relevant information from knowledge base"""
    try:
        query_vec = vectorizer.transform([query])
        similarity_scores = cosine_similarity(query_vec, kb_vectors)[0]
        
        # Get the indices of top_k most similar documents
        top_indices = similarity_scores.argsort()[-top_k:][::-1]
        
        if similarity_scores[top_indices[0]] > 0.1:  # Threshold for relevance
            results = [kb[i] for i in top_indices]
            return "\n\n".join(results)
        else:
            # If no good match, generate a response
            return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„Ùƒ."
    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}"

# ---------------------------
# Streamlit UI with Chat Interface
# ---------------------------
def main():
    st.set_page_config(
        page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ", 
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat messages from history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Accept user input
    if prompt := st.chat_input("Ø£ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message in chat container
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Display assistant response in chat container
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±...")
            
            # Process the request
            response = process_request(prompt)
            
            # Update with the full response
            message_placeholder.markdown(response)
            
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})

    # Sidebar with examples and information
    with st.sidebar:
        st.header("ğŸŒŸ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯")
        st.markdown("""
        **Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ** ÙŠÙ…ÙƒÙ†Ù‡:
        
        * ğŸ”¤ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        * ğŸ“ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        * ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„Ù†Øµ
        * ğŸ—£ï¸ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡Ø¬Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        * ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        """)
        
        st.header("ğŸ” Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
        example1 = "ØªØ±Ø¬Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØºÙŠØ± Ø§Ù„Ø¹Ø§Ù„Ù… Ø¨Ø³Ø±Ø¹Ø© ÙƒØ¨ÙŠØ±Ø©"
        example2 = "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"
        example3 = "Ø­Ù„Ù„ Ù…Ø´Ø§Ø¹Ø± Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§ Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¦Ø¹"
        example4 = "Ù…Ø§ Ù‡ÙŠ Ù„Ù‡Ø¬Ø© ÙƒÙŠÙÙƒ Ø´Ùˆ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŸ"
        
        if st.button("ØªØ±Ø¬Ù…Ø© Ù†Øµ"):
            st.session_state.messages.append({"role": "user", "content": example1})
        
        if st.button("Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"):
            st.session_state.messages.append({"role": "user", "content": example2})
            
        if st.button("ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø±"):
            st.session_state.messages.append({"role": "user", "content": example3})
            
        if st.button("ÙƒØ´Ù Ù„Ù‡Ø¬Ø©"):
            st.session_state.messages.append({"role": "user", "content": example4})

def process_request(user_input):
    """Process user input and return appropriate response"""
    user_input = user_input.strip()
    
    # Check for empty input
    if not user_input:
        return "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©."
    
    # Process translation requests
    if user_input.startswith("ØªØ±Ø¬Ù…"):
        text_to_translate = user_input.replace("ØªØ±Ø¬Ù…", "", 1).strip()
        if text_to_translate:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©..."):
                translation = translate_text(text_to_translate)
            return f"**Ø§Ù„ØªØ±Ø¬Ù…Ø©:** \n\n{translation}"
        else:
            return "ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø¹Ø¯ ÙƒÙ„Ù…Ø© 'ØªØ±Ø¬Ù…'."
    
    # Process summarization requests
    elif user_input.startswith("Ù„Ø®Øµ") or user_input.startswith("Ù…Ù„Ø®Øµ"):
        text_to_summarize = user_input.replace("Ù„Ø®Øµ", "", 1).replace("Ù…Ù„Ø®Øµ", "", 1).strip()
        if text_to_summarize:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙ„Ø®ÙŠØµ..."):
                summary = summarize_text(text_to_summarize)
            return f"**Ø§Ù„Ù…Ù„Ø®Øµ:** \n\n{summary}"
        else:
            return "ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„Ù„ØªÙ„Ø®ÙŠØµ Ø¨Ø¹Ø¯ ÙƒÙ„Ù…Ø© 'Ù„Ø®Øµ' Ø£Ùˆ 'Ù…Ù„Ø®Øµ'."
    
    # Process sentiment analysis requests
    elif any(word in user_input for word in ["Ø´Ø¹ÙˆØ±", "Ù…Ø´Ø§Ø¹Ø±", "Ø­Ù„Ù„"]):
        cleaned_input = user_input
        for term in ["Ø´Ø¹ÙˆØ±", "Ù…Ø´Ø§Ø¹Ø±", "Ø­Ù„Ù„"]:
            cleaned_input = cleaned_input.replace(term, "")
        
        text_to_analyze = cleaned_input.strip()
        if text_to_analyze:
            sentiment = analyze_sentiment(text_to_analyze)
            return f"**ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:** \n\n{sentiment}"
        else:
            return "ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±."
    
    # Process dialect identification requests
    elif "Ù„Ù‡Ø¬Ø©" in user_input:
        text_to_analyze = user_input.replace("Ù„Ù‡Ø¬Ø©", "").strip()
        if text_to_analyze:
            dialect = detect_dialect(text_to_analyze)
            return f"**Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©:** \n\n{dialect}"
        else:
            return "ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„Ù‡Ø¬Ø©."
    
    # For all other queries, use the knowledge base
    else:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©..."):
            response = knowledge_retrieval(user_input)
        return f"**Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:** \n\n{response}"

if __name__ == "__main__":
    main()
